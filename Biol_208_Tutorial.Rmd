---
title: "208_Tutorial"
author: "Megan, Justin, Katie E"
date: "1/29/2019"
output:
  pdf_document: default
  html_document: default
---

Welcome to R coding in 208!
We will be providing a walkthrough of general R skills including: package loading, data visualization using ggplot2, and data analysis. Additionally we will introduce several packages that can aid in investigation of specific 208 topics and questions. This tutorial is split into three parts:
  Part 1 - R review
  Part 2 - Shannon-Diversity Index
  Part 3 - Survivorship Curves


#Part 1: R Review
In this part, we will cover how to install data visualization packages and use those to analyze data to test hypotheses. If you have done a tutorial of R in your previous core courses, this should be mainly review. 

We will be using ggplot2 and UsingR, and we can install both using this chunk of code. The install.packages function installs the package and the library function makes sure the package is "turned on" once you have it in R. The helpful bit of code at the beginning before install.packages will check your library and only install the package if it does not find it already installed.

```{r}
if (!require("ggplot2")) install.packages("ggplot2"); library(ggplot2)
if (!require("UsingR")) install.packages("UsingR"); library(UsingR)
```

<<<<<<< HEAD
The datasets function allows us to access the datasets already available in R. Head shows the first several lines and tail shows the last several lines. Running both are useful to begin to investigate your data, confirm the dataset is complete, and check that each column has the same number of rows. Summary will provide the five number summary for each variable in the data set and will let you know if there is any missing data ("NA"s).
=======

Datasets allows us to access the datasets already available in R. Head shows the first several lines and tail shows the last several lines. Running both are useful to begin to investigate your data, confirm the dataset is complete, and check that each column has the same number of rows. Summary will provide the five number summary for each variable in the data set and will let you know if there is any missing data ("NA"s).
>>>>>>> 255759eb7265cd2ceebb33cac1672d9d42367e10

```{r setup, include=TRUE}
datasets::iris
head(iris)
tail(iris)
summary(iris)
```

<<<<<<< HEAD
Before running any other tests or visual analyses of your data, you should first look at the data (which can be done in the environment tab) and think about possible hypotheses that you could test. evnatually generating one hypothesis to test. In this example we will be testing the hypothesis that longer sepal lengths are correlated with longer petal lengths. Since the species of each flower could have an effect on petal and sepal length, we will want to look at the relationship between these variables for each species individually. To do this, we will want to split the data up by species, which we can do by creating a subset for each one. 
=======

Before running any other tests or visual analyses of your data, you should generate a hypothesis. In this example we will be testing the hypothesis that longer sepal lengths are correlated with longer petal lengths. Since the species of each flower could have an effect on petal and sepal length, we will want to look at the relationship between these variables for each species individually. To do this, we will want to split the data up by species, which we can do by creating a subset for each one. 
>>>>>>> 255759eb7265cd2ceebb33cac1672d9d42367e10

```{r Subset}
setosa <- subset(iris,Species == "setosa")
versicolor <- subset(iris,Species == "versicolor")
virginica <- subset(iris,Species == "virginica")
```
These subsets that you just created can also be viewed within the environment tab.


Next, since these variables are quantitative, you want to run the simple.eda function on each variable from the main data set not the subsets, to determine if your data is normally distributed. This function will display a histogram, boxplot, and QQ plot for each subset. If the data is normally distributed, then it is possible to use parametric tests to test the hypotheses - on a histogram, this looks like a curve where the middle is the highest and the two sides fall away evenly. If the points follow the diagonal line in the QQ plot, then the data is most likely normally distributed.

```{r simple.eda}
simple.eda(iris$Sepal.Length)
simple.eda(iris$Petal.Length)
```
Looking at the QQ plots, we can see that the data may not normally distributed (doesnt follow the diagnol line).

<<<<<<< HEAD
Since it is hard to accurately determine sometimes if a data set is normally distributed by looking at the histogram and QQ plot we will run a shapiro test to test the null hypothesis that the data is normally distributed. 

```{r}
shapiro.test(iris$Sepal.Length)
shapiro.test(iris$Petal.Length)
```

Since both p-values are less than 0.05, we reject the null hypothesis that the data is normally distributed. Since we reject the null hypothesis you can transform the data using log10 (using the function log actually runs the natural log) to try and have your data be normally distributed.

```{r}
simple.eda(log10(iris$Sepal.Length))
simple.eda(log10(iris$Petal.Length))
shapiro.test(log10(iris$Sepal.Length))
shapiro.test(log10(iris$Petal.Length))
```


To look at the distribution of species we will use a bar graph and a table since this is a categorical variable. We ran the code for both a table and bar graph so you can see the code required. However, both are not always required. Bar graphs are helpful for visiualizing general trends in very large data sets and tables are helpful to get the actual count of a specific category.
=======

To look at the distribution of species we will use a bar graph and a table since this is a categorical variable. 
>>>>>>> 255759eb7265cd2ceebb33cac1672d9d42367e10

```{r bar graph}
ggplot(iris) + 
  aes(x=factor(Species), fill=Species) +
  geom_bar() +
  theme_classic() +
  xlab ("Species of Iris") +
  ylab("Count") +
  theme(legend.position = "false") +
  scale_fill_hue(c = 50, l = 70, h=c(90, -30))
  
```

This will allow us to visualize the data in table form. 

```{r}
table(iris$Species)
```

You can also use pie charts to display categorical data. However, because it's hard to usefully distinguish between the sizes of each wedge of the pie, they are usually not used. Bar graphs and tables are much more useful in displaying the distribution of a categorical variable.


Since petal length was not normally distributed after transforming the data, we will create a scatterplot only to help us visualize the data and not to test the relationship. One thing to note is that since the data is not normally distributed we set standard error to false so that it does not display the standard error bars around the line. If the data had been normally distributted you would set se= TRUE to include it.

```{r}
ggplot(iris, aes(x=Sepal.Length, y =Petal.Length))+
  geom_point(aes(colour = factor(Species)), shape =1, size=3)+
  theme_classic() +
  geom_smooth(method=lm, color="blue", se=FALSE) +
  xlab("Sepal Length")+
  ylab("Petal Length")+
  ylim(0,8)+
  labs(color = "Species of Iris") +
  scale_color_hue(l = 90, c = 200)
```

However, to determine if there is a signifcant relationship between the two variables we will have to run a non-parametric test. A non-parametric test is run when the varibales in your data arent normally distributed. A mann whitney u test, which is run using the following chunk of code, will work here. 

```{r}
wilcox.test(setosa$Petal.Length, setosa$Sepal.Length)
wilcox.test(versicolor$Petal.Length, versicolor$Sepal.Length)
wilcox.test(virginica$Petal.Length, virginica$Sepal.Length)
```

Since all three p-values are less than 0.05, we reject the null hypothesis. We have sufficient evidence that sepal length affects petal length for each species. 

There were many other variables in the data set for Iris. Pick different variables, create your own hypothesis, and explore and test the significance of the data.

##Question, Hypothesis, Prediction


##What subset of the data are you going to look into


##Testing normality of data


##Data visualization


##Statistical Tests

#Part 2: Shannon Diversity Index
We can also use R to investigate and explore other datasets, as well as answer different types of questions. For example, we can use R to investigate species diversity. One measure of diversity we learn about in 208 is the Shannon Diversity index. Conveniently, there is a package in R (vegan) specifically designed to calculate not only the Shannon Diversity index, but to also investigate other measures of diversity.

```{r Preliminary Steps of Data Preparation}

#first need to install and load vegan 
if (!require("vegan")) install.packages("vegan"); library(vegan)

#Using a preloaded dataset BCI
data("BCI")

#learn more about this specific dataset by going to the console and typing "??BCI"

#look at BCI dataset
BCI

#investigate this dataset
ncol(BCI)
colnames(BCI)
nrow(BCI)
rownames(BCI)

```

After learning more about the dataset we can now use vegan to run a Shannon-diversity index to determine how diverse the Barro Colorado Island plots are!

```{r Shannon-diversity index}

#Two different ways to get shannon-diversity index for each site
diversity(BCI)

#This second method is more complex, but allows us to choose other possible indices to use instead of Shannon. Here we are going to assign our calculation to a new dataset labelled diversity_dataset
diversity_dataset <- diversity(BCI, index= "shannon", MARGIN = 1, base = exp(1))
diversity_dataset

#checking the normality of this data
qqnorm(diversity_dataset)
qqline(diversity_dataset)

#using a histogram to visualize data
hist(diversity_dataset)

#investigate diversity_dataset
plot(diversity_dataset, col = "darkslategray3", lwd = 3)

summary(diversity_dataset)
ggplot(BCI)

#getting the median diversity
median(diversity_dataset)

#getting the average diversity
mean(diversity_dataset)

```

While these plots, and numbers help us get a better grasp of what the data looks like- it isn't the most useful for this dataset. If we are specifically interested in diversity we have to truly consider our axes and what we want our figures to be displaying.

Another way to visualize this data dataset is by creating a species accumulation curve.  

```{r Species Accumulation Calculation}
#Species accumulation curves allow us to visualize how many species are present for a certain number of sites. Among other functions, it allows us to determine if species count is increasing with sampling effort (number of sites).
species_accumulation <- specaccum(BCI)

#print this out to see what the data looks like
species_accumulation
```


After calculating species accumulation, we can proceed with actually visualizing the data.  

```{r Species Accumulation Curve}

#Plotting a basic species accumulation curve, again we can use plot()
plot(species_accumulation) 

#We can edit this graph to better visualize the data, and to be more visually appealing. The aesthetic options in this basic R function, plot(), vary slightly than the ones you have become familiar with in ggplot2 so we have included a list of the new options below.

#ci.type = bar, line, or polygon; Type of confidence interval drawn on the figure. 
#col=""; color of species accumulation line
#lwd= ; width of line
#ci.lty= 0,1,2,3... ; Type of border on confidence polygon or confidence lines
#ci.col=""; color for confidence line or polygon
plot(species_accumulation, ci.type="poly", col="blue", lwd=3, ci.lty=0, ci.col="orange", ylab ="Species Accumulation") 
```

Now it's your turn to further investigate this dataset (BCI). Your goals are to subset the dataframe, and then to come up with a hypothesis that you can test using the skills from Part I and Part II of this tutorial. 

##Question, Hypothesis, Prediction


##What subset of the data are you going to look into


##Testing normality of data


##Data visualization


##Statistical Tests


#Part 3: Survivorship Curves
Another topic covered during some years of 208 is mortality and survivorship. Survivorship curves indicate the proportion of a population that is still alive after a certain period of time. Raw data is usually collected as the age of the individuals when they die. You can then make a curve by plotting the proportion of individuals still alive at a given age. This allows you to compare the survivorship of different populations or of one population over time. 

The survival package in R has some useful datasets that can be used to practice making a survival curve. First, we can load the package and then pick the specific dataset, lung, to use for this example. Again, we can use head, tail, and summary to get a sense of the data.

```{r Survival Package and Lung Data}
# Load survival
library(survival)

# See the datasets in the survival package
data(package = "survival")

# Load the lung data
data(lung)

# First 5 rows of the lung data
head(lung)
tail(lung)
summary(lung)
```

The lung dataset shows the survival of patients with advanced lung cancer. 

Inst = institution code
Time = survival time (days)
Status = censoring status (1 = censored, 2 = dead)
Age = age (years)
Sex = sex (1 = male, 2 = female)
ph.ecog =	ECOG performance score (0=good 5=dead)
ph.karno =	Karnofsky performance score (bad=0-good=100) rated by physician
pat.karno =	Karnofsky performance score as rated by patient
meal.cal =	Calories consumed at meals
wt.loss =	Weight loss in last six months

Notice in the summary tab that there is some missing data for the bottom row of variables, as indicated by the NA's. If this was in data that we cared about, it would be important to only use the individuals for which we have data; however, since we don't need any of that information, we can continue without worrying about missing data.

First, we'll add a column to the dataset that has the survival time for individuals that have died: 

  dataset$columnName <- with(dataset that the information is coming from, desired information)

```{r}
# Add survival object column
lung$SurvObj <- with(lung, Surv(time, status == 2))

# Check data -- scroll to the far right to make sure it's been added
head(lung)
```

The Kaplan-Meier analysis allows us to estimate the population survival over time from a sample, even if patients drop out of the test or are studied for different lengths of time. We'll run this analysis on the data in two ways: first, we'll look at the data as a whole, and second, we'll divide the data by sex.

survfit is a function that creates survival curves from a formula (like Kaplan-Meier):

  object <- survfit(dependent variable ~ independent variable, data = dataset, conf.type = 
  confidence interval)

km.by.sex, then, is assigned the value of the results of the analysis of sex on survival, using data from the lung dataset, and log-log confidence intervals

```{r}
# Kaplan-Meier analysis
km.as.one <- survfit(SurvObj ~ 1, data = lung, conf.type = "log-log")
km.by.sex <- survfit(SurvObj ~ sex, data = lung, conf.type = "log-log")

# Show object
km.as.one
km.by.sex
```

Now we can make our plot. GGplot is a good package to use to make plots, but it's also possible to use the base package that comes with R. Since we showed you how to use ggplot above, we'll use the base package now. 

The primary black line that appears is the curve, while the two dashed lines are the confidence intervals.

```{r Plot km as one}
plot(km.as.one, xlab="Days",ylab="Proportion Surviving")
```

We can also graph the km.by.sex data, and this plot will need a legend to differentiate between the two curves.

```{r Plot km by sex}
plot(km.by.sex,col=c('blue','red'),xlab="Days",ylab="Proportion Surviving")
legend("right",legend = c("Male","Female"), col=c("blue","red"), lty = 1)
title("Kaplan-Meier Curves \n by Sex") #\n indicates a line break
```


Adapted from
https://rstudio-pubs-static.s3.amazonaws.com/5588_72eb65bfbe0a4cb7b655d2eee0751584.html


---- Human Demography Project

Sometimes you may be given the number of individuals in a given age class that died, rather than the ages at which different individuals died, such as in this human demography data from a lab run in previous years. Since it was collected, this data does not come from an R package but rather from an Excel file. To load that data into R, we need to install the readxl package. Then we can create an object (demographyData) in R that has the information from the Excel file. Viewing that object lets us see the dataset.
  
```{r Upload Data }
install.packages("readxl")
if (!require("tidyverse")) install.packages("tidyverse"); library(tidyverse)
library(readxl)
library(ggplot2)
library(cowplot)
library(plyr)


demographyData <- read_excel("BIO208_Demog_Practice.xlsx")
#View(demographyData)
names(demographyData) <- make.names(names(demographyData))

```

In the above survival curve, it looked like males had a steeper curve than females. Let's see if that pattern holds for this data. To do that, we'll need to subset out the data both by year and by sex - for the sake of simplicity, we can just use the pre-1840 data for men and women.

```{r}
DemographySubset <- dplyr::select(demographyData,c(Age.Class,Pre.1840.Female,Pre.1840.Male))
```


Then we'll need to calculate the proportion of individuals alive for each age class. Since the data we're given is the number of people that have died for each age class, we'll have to do some math. First, we need to calculate the number of people that are still alive for each age class. Then we'll need to use that information to calculate the proportion of people that are still alive for each age class.


```{r Calculate Proportions}
#Create a new column of the number of dead people by cumulatively adding the rows of data
DemographySubset$P1840FNumDead <- cumsum(DemographySubset$Pre.1840.Female)
#Create a new column of the number of people alive, with the first row as the total
DemographySubset$P1840FNumAlive[1] <- sum(DemographySubset$Pre.1840.Female)
#The consecutive rows are that total - the number of people dead
DemographySubset$P1840FNumAlive[2:20] <- sum(DemographySubset$Pre.1840.Female) - DemographySubset$P1840FNumDead[1:19]
#Create a new column of the proportion of people alive 
DemographySubset$P1840FPropAlive <- DemographySubset$P1840FNumAlive/DemographySubset$P1840FNumAlive[1]

DemographySubset$P1840MNumDead <- cumsum(DemographySubset$Pre.1840.Male)
DemographySubset$P1840MNumAlive[1] <- sum(DemographySubset$Pre.1840.Male)
DemographySubset$P1840MNumAlive[2:20] <- sum(DemographySubset$Pre.1840.Male) - DemographySubset$P1840MNumDead[1:19]
DemographySubset$P1840MPropAlive <- DemographySubset$P1840MNumAlive/DemographySubset$P1840MNumAlive[1]

```

Then we can use ggplot2 to plot survivorship curves of the data. We'll have to specify the order of the age classes on the x axis, because otherwise ggplot will plot them in numeric order (where the age class of 6-10 will follow 56-60 rather than 0-5)

```{r}
ggplot(DemographySubset) +
  geom_point(aes(x = factor(Age.Class, levels = c("0-5","6-10", "11-15", "16-20", "21-25", "26-30", "31-35", "36-40", "41-45", "46-50", "51-55", "56-60", "61-65", "66-70", "71-75","76-80", "81-85", "86-90", "91-95", "96+")), y = as.numeric(P1840FPropAlive), color="red")) +
  geom_point(aes(x = factor(Age.Class, levels = c("0-5","6-10", "11-15", "16-20", "21-25", "26-30", "31-35", "36-40", "41-45", "46-50", "51-55", "56-60", "61-65", "66-70", "71-75","76-80", "81-85", "86-90", "91-95", "96+")), y = as.numeric(P1840MPropAlive),color="blue")) +
  theme(legend.position = "right")  +
  scale_color_manual(labels = c("Pre-1840 Female", "Pre-1840 Male"), values = c("red", "blue")) +
  theme_cowplot() +
  xlab("Age Class") + 
  theme(axis.text.x = element_text(angle = 35, hjust = 1)) +
  ylab("Survivorship")


```



#Trouble shooting
1.) It is essential you load packages prior to using the functions contained within them. Error messages that report "object" or "function" not found can often be traced back to improper loading of packages or not running the necessary steps beforehand (check your 'Environment' to be sure all datasets and variables are loaded). 
2.) Syntax is especially important while coding in R. If not all of your graph changes are being incorporated be sure you have included "+" for each new line addition. Also be sure to close all parenthetical expressions properly. If your error reports an unexpected symbol, you may want to start by checking to make sure you have enough parenthesis included.
3.) Use the R help section to learn what each function does, and how to set it up properly. You should also confirm your data conforms to the proper structure needed for that function. 
4.) A very important thing to note is that it is very important to know what your null hypothesis is when running a test. R is a very powerful program that lets you run many different statistical tests however it is not very intuitive. Generally in stats, when you get a p-value of 0.05 that means the relationship you are testing is significant so intuitively when you run the shapiro test an easy mistake to make is that a p value of 0.05 means that your data is normally distributed. However, here the null hypothesis is that your data is normally distributed, which means that rejecting the null hypothesis here (p value < 0.05) means the data is NOT normally distributed.

#Other resources
For more guidance in using R, there are many resources available on the internet, such as Stack Overflow. However, we will first refer you to the other tutorials you may or may not have taken in your previous core biology courses (205-207) as they lead up to the skills and ideas used in this tutorial. 

#Acknowledgments
We would like to acknowledge the use of several resources cited throughout this tutorial and stack overflow for aiding us. Additionally, we would like to reference the tutorials for the 205-207 courses that we referenced to determine what skill and experience level we should be aiming for. We divided the work put into this tutorial amongst group members as follows:
Justin: R Basics/Review
Megan: Shannon Diversity & scatterplot used in Part I
Katie: Survivorship Curves
We all contributed notes throughout our own sections, and contributed thoughtfully to how the tutorial was structured in terms of complexity and subject material. We also all looked over each others sections to make sure the tutorial was succint and built well off of previous sections.

We would also like to acknowledge the use of google, and stack overflow. The following websites/blogs/tutorials were also used:
http://www.flutterbys.com.au/stats/tut/tut13.2.html




