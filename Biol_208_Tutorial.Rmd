---
title: "208_Tutorial"
author: "Megan, Justin, Katie E"
date: "1/29/2019"
output:
  pdf_document: default
  html_document: default
---

Welcome to R coding in 208!
We will be providing a walkthrough of general R skills including: package loading, data visualization using ggplot2, and data analysis. Additionally we will introduce several packages that can aid in investigation of specific 208 topics and questions. This tutorial is split into three parts:
  Part 1 - R review
  Part 2 - Shannon-Diversity Index
  Part 3 - Survivorship Curves


#Part 1: R Review
In this part, we will cover how to install data visualization packages and use those to analyze data to test hypotheses. If you have done a tutorial of R in your previous core courses, this should be mainly review. 

We will be using ggplot2 and UsingR, and we can install both using this chunk of code. The install.packages function installs the package and the library function makes sure the package is "turned on" once you have it in R. The helpful bit of code at the beginning before install.packages will check your library and only install the package if it does not find it already installed.
```{r}
if (!require("ggplot2")) install.packages("ggplot2"); library(ggplot2)
if (!require("UsingR")) install.packages("UsingR"); library(UsingR)
```

Datasets allows us to access the datasets already available in R. Head shows the first several lines and tail shows the last several lines. Running both are useful to begin to investigate your data, confirm the dataset is complete, and check that each column has the same number of rows. Summary will provide the five number summary for each variable in the data set and will let you know if there is any missing data ("NA"s).

```{r setup, include=TRUE}
library(ggplot2)
datasets::iris
head(iris)
tail(iris)
summary(iris)
```

Before running any other tests or visual analyses of your data, you should generate a hypothesis. In this example we will be testing the hypothesis that longer sepal lengths are correlated with longer petal lengths. Since the species of each flower could have an effect on petal and sepal length, we will want to look at the relationship between these variables for each species individually. To do this, we will want to split the data up by species, which we can do by creating a subset for each one. 

```{r Subset}
setosa <- subset(iris,Species == "setosa")
versicolor <- subset(iris,Species == "versicolor")
virginica <- subset(iris,Species == "virginica")
```

Next, since these variables are quantitative, you want to run the simple.eda function on each variable from the main data set not the subsets, to determine if your data is normally distributed. This function will display a histogram, boxplot, and QQ plot for each subset. If the data is normally distributed, then it is possible to use parametric tests to test the hypotheses - on a histogram, this looks like a curve where the middle is the highest and the two sides fall away evenly. If the points follow the diagonal line in the QQ plot, then the data is most likely normally distributed.

```{r simple.eda}
simple.eda(iris$Sepal.Length)
simple.eda(iris$Petal.Length)
```

To look at the distribution of species we will use a bar graph and a table since this is a categorical variable. 

```{r bar graph}
ggplot(iris) + 
  aes(x=factor(Species), fill=Species) +
  geom_bar() +
  theme_classic() +
  xlab ("Species of Iris") +
  ylab("Count") +
  theme(legend.position = "false") +
  scale_fill_hue(c = 50, l = 70, h=c(90, -30))
  
```


```{r}
table(iris$Species)
```


You can also use pie charts to display categorical data. However, because it's hard to usefully distinguish between the sizes of each wedge of the pie, they are usually not used. Bar graphs and tables are much more useful in displaying the distribution of a categorical variable.

Since it is hard to accurately determine sometimes if a data set is normally distributed by looking at the histogram and QQ plot we will run a shapiro test to test the null hypothesis that the data is normally distributed. 

```{r}
shapiro.test(iris$Sepal.Length)
shapiro.test(iris$Petal.Length)
```

Since both p-values are less than 0.05, we reject the null hypothesis that the data is normally distributed. Since we reject the null hypothesis you can transform the data using log10 (using the function log actually runs the natural log) to try and have your data be normally distributed.

```{r}
simple.eda(log10(iris$Sepal.Length))
simple.eda(log10(iris$Petal.Length))
shapiro.test(log10(iris$Sepal.Length))
shapiro.test(log10(iris$Petal.Length))
```

Since petal length was not normally distributed after transforming the data, we will create a scatterplot only to help us visualize the data and not to test the relationship. One thing to note is that since the data is not normally distributed we set standard error to false so that it does not display the standard error bars around the line. If the data had been normally distributted you would set se= TRUE to include it.

```{r}
ggplot(iris, aes(x=Sepal.Length, y =Petal.Length))+
  geom_point(aes(colour = factor(Species)), shape =1, size=3)+
  theme_classic() +
  geom_smooth(method=lm, color="blue", se=FALSE) +
  xlab("Sepal Length")+
  ylab("Petal Length")+
  ylim(0,8)+
  labs(color = "Species of Iris") +
  scale_color_hue(l = 90, c = 200)
```

However, to determine if there is a signifcant relationship between the two variables we will have to run a non-parametric test. A mann whitney u test, which is run using the following chunk of code, will work here. 

```{r}
wilcox.test(setosa$Petal.Length, setosa$Sepal.Length)
wilcox.test(versicolor$Petal.Length, versicolor$Sepal.Length)
wilcox.test(virginica$Petal.Length, virginica$Sepal.Length)
```

Since all three p-values are less than 0.05, we reject the null hypothesis. We have sufficient evidence that sepal length affects petal length for each species. There were other variables in the data set for Iris. Pick different variables, create your own hypothesis, and explore and test the significance of the data.



#Part 2: Shannon Diversity Index
We can also use R to investigate and explore other datasets, as well as answer different types of questions. For example, we can use R to investigate species diversity. One measure of diversity we learn about in 208 is the Shannon Diversity index. Conveniently, there is a package in R (vegan) specifically designed to calculate not only the Shannon Diversity index, but to also investigate other measures of diversity.

```{r Preliminary Steps of Data Preparation}

#first need to install and load vegan 
if (!require("vegan")) install.packages("vegan"); library(vegan)

#Using a preloaded dataset BCI
data("BCI")

#learn more about this specific dataset by going to the console and typing "??BCI"

#look at BCI dataset
BCI

#investigate this dataset
ncol(BCI)
colnames(BCI)
nrow(BCI)
rownames(BCI)

```

After learning more about the dataset we can now use vegan to run a Shannon-diversity index to determine how diverse the Barro Colorado Island plots are!

```{r Shannon-diversity index}

#Two different ways to get shannon-diversity index for each site
diversity(BCI)

#This second method is more complex, but allows us to choose other possible indices to use instead of Shannon. Here we are going to assign our calculation to a new dataset labelled diversity_dataset
diversity_dataset <- diversity(BCI, index= "shannon", MARGIN = 1, base = exp(1))
diversity_dataset

#checking the normality of this data
qqnorm(diversity_dataset)
qqline(diversity_dataset)

#using a histogram to visualize data
hist(diversity_dataset)

#investigate diversity_dataset
plot(diversity_dataset)
summary(diversity_dataset)

#getting the median diversity
median(diversity_dataset)

#getting the average diversity
mean(diversity_dataset)

```

Another way to visualize this data dataset is by creating a species accumulation curve.  

```{r Species Accumulation Curve}
#Species accumulation curves allow us to visualize how many species are present for a certain number of sites. Among other functions, it allows us to determine if species count is increasing with sampling effort (number of sites).
species_accumulation <- specaccum(BCI)

#Plotting a basic species accumulation curve
plot(species_accumulation) 

#We can edit this graph to better visualize the data, and to be more visually appealing. The aesthetic options in this basic R function, plot(), vary slightly than the ones you have become familiar with in ggplot2 so we have included a list of the new options below.

#ci.type = bar, line, or polygon; Type of confidence interval drawn on the figure. 
#col=""; color of species accumulation line
#lwd= ; width of line
#ci.lty= 0,1,2,3... ; Type of border on confidence polygon or confidence lines
#ci.col=""; color for confidence line or polygon
plot(species_accumulation, ci.type="poly", col="blue", lwd=3, ci.lty=0, ci.col="orange", ylab ="Species Accumulation") 
```


#Part 3: Survivorship Curves
Survivorship curves indicate the proportion of a population that is still alive after a certain period of time. Raw data is usually collected as the age of the individuals when they die. You can then make a curve by plotting the proportion of individuals still alive at a given age.

The survival package in R has some useful datasets that can be used to practice making a survival curve. First, we can load the package and then pick the specific dataset, lung, to use for this example. Again, we can use head, tail, and summary to get a sense of the data.

```{r Survival Package and Lung Data}
# Load survival
library(survival)

# See the datasets in the survival package
data(package = "survival")

# Load the lung data
data(lung)

# First 5 rows of the lung data
head(lung)
tail(lung)
summary(lung)
```

The lung dataset shows the survival of patients with advanced lung cancer. 

Inst = institution code
Time = survival time (days)
Status = censoring status (1 = censored, 2 = dead)
Age = age (years)
Sex = sex (1 = male, 2 = female)
ph.ecog =	ECOG performance score (0=good 5=dead)
ph.karno =	Karnofsky performance score (bad=0-good=100) rated by physician
pat.karno =	Karnofsky performance score as rated by patient
meal.cal =	Calories consumed at meals
wt.loss =	Weight loss in last six months

Notice in the summary tab that there is some missing data for the bottom row of variables, as indicated by the NA's. If this was in data that we cared about, it would be important to only use the individuals for which we have data; however, since we don't need any of that information, we can continue without worrying about missing data.

First, we'll add a column to the dataset that has the survival time for individuals that have died: 

  dataset$columnName <- with(dataset that the information is coming from, desired information)

```{r}
# Add survival object column
lung$SurvObj <- with(lung, Surv(time, status == 2))

# Check data -- scroll to the far right to make sure it's been added
head(lung)
```

The Kaplan-Meier analysis allows us to estimate the population survival over time from a sample, even if patients drop out of the test or are studied for different lengths of time. We'll run this analysis on the data in two ways: first, we'll look at the data as a whole, and second, we'll divide the data by sex.

survfit is a function that creates survival curves from a formula (like Kaplan-Meier):

  object <- survfit(dependent variable ~ independent variable, data = dataset, conf.type = 
  confidence interval)

km.by.sex, then, is assigned the value of the results of the analysis of sex on survival, using data from the lung dataset, and log-log confidence intervals

```{r}
# Kaplan-Meier analysis
km.as.one <- survfit(SurvObj ~ 1, data = lung, conf.type = "log-log")
km.by.sex <- survfit(SurvObj ~ sex, data = lung, conf.type = "log-log")

# Show object
km.as.one
km.by.sex
```

Now we can make our plot. GGplot is a good package to use to make plots, but it's also possible to use the base package that comes with R. Since we showed you how to use ggplot above, we'll use the base package now. 

The primary black line that appears is the curve, while the two dashed lines are the confidence intervals.

```{r Plot km as one}
plot(km.as.one, xlab="Days",ylab="Proportion Surviving")
```

We can also graph the km.by.sex data, and this plot will need a legend to differentiate between the two curves.

```{r Plot km by sex}
plot(km.by.sex,col=c('blue','red'),xlab="Days",ylab="Proportion Surviving")
legend("right",legend = c("Male","Female"), col=c("blue","red"), lty = 1)
title("Kaplan-Meier Curves \n by Sex") #\n indicates a line break
```


Adapted from
https://rstudio-pubs-static.s3.amazonaws.com/5588_72eb65bfbe0a4cb7b655d2eee0751584.html




---- Human Demography Project

Sometimes the data does not come from an R package but rather from an Excel file. To load that data into R, we need to install the readxl package. Then we can create an object (demographyData) in R that has the information from the Excel file. Viewing that object lets us see the dataset.
  
```{r Upload Data }
install.packages("readxl")
library(readxl)
library(ggplot2)
library(cowplot)


demographyData <- read_excel("BIO208_Demog_Calcs_Master.xlsx")
View(demographyData)

```

In the above survival curve, it looked like males had a steeper curve than females. Let's see if that pattern holds for this data. To do that, we'll need to subset out the data both by year and by sex - for the sake of simplicity, we can just use the pre-1840 data for men and women.


```{r Subsets}
fpre1840 <- subset(demographyData, select = "Pre-1840 Female")
mpre1840 <- subset(demographyData, select = "Pre-1840 Male")

```

Then we'll need to calculate the proportion of individuals alive for each age class. Since the data we're given is the number of people that have died for each age class, we'll have to do some math. First, we need to calculate the number of people that are still alive for each age class. Then we'll need to use that information to calculate the proportion of people that are still alive for each age class.

Since we'll want to run the same algorithm multiple times, it makes sense to write a couple functions that can do this math for us. 

```{r Functions}

#function that calculates the number of people still alive
numStillAlive <- function(subset1) { 
  totalStartAlive <- sum(subset1) #find the total number of people alive
  for (i in row(subset1)){ #for every row (age class)
    sumDead <- sum(subset1[1:i,1]) #add every row including that one to find the total dead
    stillAlive <- totalStartAlive - sumDead #subtract total dead from total alive to find still alive
    listStillAlive <- c(listStillAlive,stillAlive) #make a list of the number of people still alive
  }
  return(listStillAlive)
}

#Function that calculates the proportion of people still alive
proportionAlive <- function(dataframe) {
  for (i in col(dataframe)) { #for every number on the list
    a <- dataframe[i]/dataframe[1] #divide that that number by the first number to find prop. alive
    list <- c(list,a) #make a list of the proportion of people alive
  }
  return(list)
}


```

Then we'll just need to run our data through the functions and use the numbers it produces. We can also add a column indicating the proportion of people still alive to each of our datasets, as well as the values that we'll want on the x axis.


```{r Calculate Proportions}
fpre1840alive <- as.data.frame(numStillAlive(fpre1840))
fpre1840prop <- proportionAlive(fpre1840alive)
fpre1840$PropAlive <- fpre1840prop[2:21]
fpre1840$xAxis <- 1:20

mpre1840alive <- as.data.frame(numStillAlive(mpre1840))
mpre1840prop <- proportionAlive(mpre1840alive)
mpre1840$PropAlive <- mpre1840prop[2:21]
mpre1840$xAxis <- 1:20

#mpre1840$xAxis <- ("0-5" "6-10" "11-15" "16-20" "21-25" "26-30" "31-35" "36-40" "41-45" "46-50" "51-55" "56-60" "61-65" "66-70" "71-75""76-80" "81-85" "86-90" "91-95" "96+")

```

Then we can use ggplot2 to plot survivorship curves of the data.

```{r Plot Survivorship}
ggplot(fpre1840) +
  aes(x = as.numeric(xAxis), y = as.numeric(PropAlive)) +
  geom_line() +
  theme_cowplot() +
  xlab("Age Class") + 
  ylab("Survivorship")

ggplot(mpre1840) +
  aes(x = as.numeric(xAxis), y = as.numeric(PropAlive)) +
  geom_line() +
  theme_cowplot() +
  xlab("Age Class") + 
  ylab("Survivorship")
```


#Trouble shooting
1.) It is essential you load packages prior to using the functions contained within them. Error messages that report "object" or "function" not found can often be traced back to improper loading of packages or not running the necessary steps beforehand (check your 'Environment' to be sure all datasets and variables are loaded). 
2.) Syntax is especially important while coding in R. If not all of your graph changes are being incorporated be sure you have included "+" for each new line addition. Also be sure to close all parenthetical expressions properly. If your error reports an unexpected symbol, you may want to start by checking to make sure you have enough parenthesis included.
3.) Use the R help section to learn what each function does, and how to set it up properly. You should also confirm your data conforms to the proper structure needed for that function. 
4.) A very important thing to note is that it is very important to know what your null hypothesis is when running a test. R is a very powerful program that lets you run many different statistical tests however it is not very intuitive. Generally in stats, when you get a p-value of 0.05 that means the relationship you are testing is significant so intuitively when you run the shapiro test an easy mistake to make is that a p value of 0.05 means that your data is normally distributed. However, here the null hypothesis is that your data is normally distributed, which means that rejecting the null hypothesis here (p value < 0.05) means the data is NOT normally distributed.

#Other resources
For more guidance in using R, there are many resources available on the internet, such as Stack Overflow. However, we will first refer you to the other tutorials you may or may not have taken in your previous core biology courses (205-207) as they lead up to the skills and ideas used in this tutorial. 

#Acknowledgments
We would like to acknowledge the use of several resources cited throughout this tutorial and stack overflow for aiding us. Additionally, we would like to reference the tutorials for the 205-207 courses that we referenced to determine what skill and experience level we should be aiming for. We divided the work put into this tutorial amongst group members as follows:
Justin: R Basics/Review
Megan: Shannon Diversity & scatterplot used in Part I
Katie: Survivorship Curves
We all contributed notes throughout our own sections, and contributed thoughtfully to how the tutorial was structured in terms of complexity and subject material. We also all looked over each others sections to make sure the tutorial was succint and built well off of previous sections.


