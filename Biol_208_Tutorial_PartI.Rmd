---
title: "208_Tutorial"
author: "Megan, Justin, Katie"
date: "2/18/2019"
output:
  html_document: default
  pdf_document: default
  toc: true
  toc_depth: 2
---

Welcome to R coding in 208!
We will be providing a walkthrough of general R skills including: package loading, data visualization using ggplot2, and data analysis. Additionally we will introduce several packages that can aid in investigation of specific 208 topics and questions. This tutorial is split into three parts:
  Part 1 - R review
  Part 2 - Shannon-Diversity Index
  Part 3 - Survivorship Curves

In this R markdown, we will be covering Part 1. 

#Introduction 
In this part, we will cover how to install data visualization packages and use those to analyze data to test hypotheses. If you have done a tutorial of R in your previous core courses, this should be mainly review. 

We will be using ggplot2 and UsingR, and we can install both using this chunk of code. The install.packages function installs the package and the library function makes sure the package is "turned on" once you have it in R. The helpful bit of code at the beginning before install.packages will check your library and only install the package if it does not find it already installed.


```{r}
if (!require("ggplot2")) install.packages("ggplot2"); library(ggplot2)
if (!require("UsingR")) install.packages("UsingR"); library(UsingR)
```


The datasets function allows us to access the datasets already available in R. Head shows the first several lines and tail shows the last several lines of the data. Running both are useful to begin to investigate your data, confirm that the dataset is complete, and check that each column has the same number of rows. Summary will provide the five number summary for each variable in the dataset and will let you know if there is any missing data ("NA"s).


```{r setup, include=TRUE}
datasets::iris
head(iris)
tail(iris)
summary(iris)
```


Before running any other tests or visual analyses of your data, you should first look at the data (which can be done by clicking on the object in the environment tab) and think about possible hypotheses that you could test. In this example we will be testing the hypothesis that longer sepal lengths are correlated with longer petal lengths. Since the species of each flower could have an effect on petal and sepal length, we will want to look at the relationship between these variables for each species individually. To do this, we will want to split the data up by species, which we can do by creating a subset for each one. 


```{r Subset}
setosa <- subset(iris,Species == "setosa")
versicolor <- subset(iris,Species == "versicolor")
virginica <- subset(iris,Species == "virginica")
```


These subsets that you just created can also be viewed within the environment tab.

Next, since these variables are quantitative, you want to run the simple.eda function on each variable from the main dataset (not the subsets) to determine if your data is normally distributed. This function will display a histogram, boxplot, and QQ plot for each subset. If the data is normally distributed, then it is possible to use parametric tests to test the hypotheses - on a histogram, this looks like a nice bell curve where the middle is the highest and the two sides fall away evenly. If the points follow the diagonal line in the QQ plot, then the data is most likely normally distributed.


```{r simple.eda}
simple.eda(iris$Sepal.Length)
simple.eda(iris$Petal.Length)
```


Looking at the QQ plots, we can see that the data may not be normally distributed.

Since it can be hard to accurately determine if a data set is normally distributed by looking at the histogram and QQ plot, we will run a shapiro test to test the null hypothesis that the data is normally distributed. 


```{r}
shapiro.test(iris$Sepal.Length)
shapiro.test(iris$Petal.Length)
```


Since both p-values are less than 0.05, we reject the null hypothesis that the data is normally distributed. Having rejected the null hypothesis, we can transform the data using log10 (using the function log actually runs the natural log) to try and have our data be normally distributed.


```{r}
simple.eda(log10(iris$Sepal.Length))
simple.eda(log10(iris$Petal.Length))
shapiro.test(log10(iris$Sepal.Length))
shapiro.test(log10(iris$Petal.Length))
```


To look at the distribution of species we will use a bar graph and a table since this is a categorical variable. We ran the code for both a table and bar graph so you can see the code required. However, both are not always required. Bar graphs are helpful for visiualizing general trends in very large data sets, and tables are helpful to get the actual count of a specific category.


```{r bar graph}
ggplot(iris) + 
  aes(x=factor(Species), fill=Species) +
  geom_bar() +
  theme_classic() +
  xlab ("Species of Iris") +
  ylab("Count") +
  theme(legend.position = "false") +
  scale_fill_hue(c = 50, l = 70, h=c(90, -30))
  
```


This will allow us to visualize the data in table form. 


```{r}
table(iris$Species)
```


You can also use pie charts to display categorical data. However, because it's hard to usefully distinguish between the sizes of each wedge of the pie, they are usually not used. Bar graphs and tables are much more useful in displaying the distribution of a categorical variable.

Since petal length was still not normally distributed after transforming the data, we will create a scatterplot only to help us visualize the data and not to test the relationship. One thing to note is that since the data is not normally distributed, we set standard error to false so that it does not display the standard error bars around the line. If the data had been normally distributted, you would set se= TRUE to include it.


```{r}
ggplot(iris, aes(x=Sepal.Length, y =Petal.Length))+
  geom_point(aes(colour = factor(Species)), shape =1, size=3)+
  theme_classic() +
  geom_smooth(method=lm, color="blue", se=FALSE) +
  xlab("Sepal Length")+
  ylab("Petal Length")+
  ylim(0,8)+
  labs(color = "Species of Iris") +
  scale_color_hue(l = 90, c = 200)
```


However, to determine if there is a signifcant relationship between the two variables we will have to run a non-parametric test. A non-parametric test is run when the varibales in your data arent normally distributed. A mann whitney u test, which is run using the following chunk of code, will work here. 


```{r}
wilcox.test(setosa$Petal.Length, setosa$Sepal.Length)
wilcox.test(versicolor$Petal.Length, versicolor$Sepal.Length)
wilcox.test(virginica$Petal.Length, virginica$Sepal.Length)
```


Since all three p-values are less than 0.05, we reject the null hypothesis. We have sufficient evidence that sepal length affects petal length for each species. 


#Creating and testing your own hypothesis

There were many other variables in the data set for Iris. Pick different variables, create your own hypothesis, and explore and test the significance of the data.

##Question, Hypothesis, Prediction


##What subset of the data are you going to look into


##Testing normality of data


##Data visualization: what graphs will you use to investigate this relationship?


##Statistical Tests



#Trouble shooting
1.) It is essential you load packages prior to using the functions contained within them. Error messages that report "object" or "function" not found can often be traced back to improper loading of packages or not running the necessary steps beforehand (check your 'Environment' to be sure all datasets and variables are loaded). 
2.) Syntax is especially important while coding in R. If not all of your graph changes are being incorporated be sure you have included "+" for each new line addition. Also be sure to close all parenthetical expressions properly. If your error reports an unexpected symbol, you may want to start by checking to make sure you have enough parenthesis included.
3.) Use the R help section to learn what each function does, and how to set it up properly. You should also confirm your data conforms to the proper structure needed for that function. 
4.) A very important thing to note is that it is very important to know what your null hypothesis is when running a test. R is a very powerful program that lets you run many different statistical tests however it is not very intuitive. Generally in stats, when you get a p-value of 0.05 that means the relationship you are testing is significant so intuitively when you run the shapiro test an easy mistake to make is that a p value of 0.05 means that your data is normally distributed. However, here the null hypothesis is that your data is normally distributed, which means that rejecting the null hypothesis here (p value < 0.05) means the data is NOT normally distributed.

#Other resources
For more guidance in using R, there are many resources available on the internet, such as Stack Overflow. However, we will first refer you to the other tutorials you may or may not have taken in your previous core biology courses (205-207) as they lead up to the skills and ideas used in this tutorial. 
